{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Writing Assistant: Playground\n\n",
    "This notebook allows you to step through the core logic of your AI Writing Assistant. You can inspect the intermediate outputs at each stage, from document retrieval to the final prompt construction and generation.\n\n",
    "**Instructions:**\n",
    "1. Make sure you have run `pip install -r requirements.txt` in your `story` virtual environment.\n",
    "2. Ensure your Ollama application is running in the background.\n",
    "3. Run the cells sequentially to see how the system works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n\n",
    "Here, we define the core components we'll be using. You can change `YOUR_PROMPT` and `LLM_MODEL` to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your Query ---\n",
    "YOUR_PROMPT = \"Write a short tutorial on how to use git rebase.\"\n",
    "\n",
    "# --- Core Components ---\n",
    "DB_PATH = \"db\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL = \"qwen2:7b-instruct-q4_K_M\" # Make sure this model is pulled in Ollama\n",
    "\n",
    "# --- Imports ---\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import box\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Knowledge Base\n\n",
    "First, we load our vector database, which contains all the processed chunks from your blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "\n",
    "print(\"Vector store loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Retriever\n\n",
    "Now, we create a `retriever`. Its job is to take your prompt, search the vector store, and find the most relevant chunks of your own writing. These chunks will be used as stylistic examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) # We'll retrieve 3 chunks for this example\n",
    "\n",
    "print(\"Retriever created. Ready to search for stylistic context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the Retrieved Context\n\n",
    "Let's test the retriever with your prompt and see what it finds. This is the raw context that will be fed to the LLM to guide its writing style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(YOUR_PROMPT)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for the prompt: '{YOUR_PROMPT}'\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"--- Retrieved Document {i+1} ---\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(doc.page_content)\n",
    "    print(\"---------------------------\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Constructing the Final Prompt\n\n",
    "This is the most critical step. We will now assemble the full prompt that gets sent to the LLM. It combines:\n",
    "1.  **Instructions:** Telling the AI its role (technical vs. creative).\n",
    "2.  **Context:** The stylistic examples we just retrieved.\n",
    "3.  **Question:** Your original prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same Technical Prompt Template from main.py\n",
    "TECHNICAL_PROMPT_TEMPLATE = \"\"\"\n",
    "**Instructions:** You are a technical writer. Your primary goal is to create a clear, informative, and well-structured technical article based on the user's request.\n",
    "Adopt the author's writing style from the examples below, focusing on how they explain complex topics.\n",
    "Prioritize clarity, accuracy, and logical structure. Use the provided context for stylistic guidance only.\n",
    "\n",
    "**Author's Style Examples (Context):**\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "**User's Request (Question):**\n",
    "{question}\n",
    "\n",
    "**Formatting Instructions:**\n",
    "Use Markdown for formatting, including headings, lists, and bold/italic text where appropriate.\n",
    "\n",
    "**Your Story:**\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=TECHNICAL_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "# Manually create the context string\n",
    "context_string = \"\n\n---\n\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "final_prompt = prompt_template.format(\n",
    "    context=context_string,\n",
    "    question=YOUR_PROMPT\n",
    ")\n",
    "\n",
    "print(\"--- FINAL PROMPT SENT TO LLM ---\")\n",
    "print(final_prompt)\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate the Story\n\n",
    "Finally, we send the fully constructed prompt to the local LLM and stream the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=LLM_MODEL, stop=[\"<|begin_of_text|>\", \"<|end_of_text|>\"])\n",
    "\n",
    "print(\"--- AI-Generated Story ---\")\n",
    "\n",
    "# Using the final_prompt we constructed above\n",
    "full_response = llm.invoke(final_prompt)\n",
    "print(full_response)\n",
    "\n",
    "print(\"\n--- End of Story ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}